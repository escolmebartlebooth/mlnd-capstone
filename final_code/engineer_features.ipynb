{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "This section takes pre-engineered lookup files and creates new features in the training data. It also transposes previous weeks' sales transactions to act as features for current week data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports and Settings\n",
    "\n",
    "# specify a data file path\n",
    "data_location = \"../data/\"\n",
    "\n",
    "# imports for data exploration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# further imports to go here\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading complete in:  1.27300000191  seconds\n"
     ]
    }
   ],
   "source": [
    "# let's now take in the smaller training data file\n",
    "train_file = \"smaller_train.csv\"\n",
    "\n",
    "# specify columns for the training data as spanish is not a strong point in London\n",
    "train_columns = ['WeekNumber','AgencyId','ChannelId','RouteId','ClientId','ProductId','Sales',\n",
    "                 'SalesPesos','Returns','ReturnsPesos','Demand']\n",
    "\n",
    "# load the training data in full, skip the column headings on the first rows\n",
    "t0 = time.time() \n",
    "df_train = pd.read_csv(data_location + train_file,names=train_columns,skiprows=1)\n",
    "print 'loading complete in: ', time.time()-t0, ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lag features\n",
    "We will create lagged sales data for 3 previous periods which will mean we will not have target labels for weeks 3,4 or 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data separated into weeks in:  0.118999958038  seconds\n",
      "lagged data created in:  3.87400007248  seconds\n",
      "loading complete in:  4.26600003242  seconds. Dataset now has: 1062620  records\n"
     ]
    }
   ],
   "source": [
    "# first separate the data into week by week dataframes\n",
    "min_week = df_train[\"WeekNumber\"].min()\n",
    "run_week = min_week\n",
    "max_week = df_train[\"WeekNumber\"].max()\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "filtered_data = {}\n",
    "while (run_week <= max_week):\n",
    "    # slice training data by week\n",
    "    df_filter = df_train[df_train['WeekNumber']==run_week]\n",
    "    \n",
    "    # store in dictionary against run week\n",
    "    filtered_data[run_week] = df_filter\n",
    "    run_week += 1\n",
    "\n",
    "t1 = time.time()\n",
    "print 'data separated into weeks in: ', t1-t0, ' seconds'\n",
    "\n",
    "# now need to create a new training set with lag weeks and tidy up\n",
    "run_week = max_week\n",
    "joined_data = []\n",
    "while (run_week-3 >= min_week):\n",
    "    # create lagged data from -1,-2,-3 for 0\n",
    "    \n",
    "    df = filtered_data[run_week]\n",
    "    \n",
    "    #lag 1\n",
    "    df_1 = filtered_data[run_week-1]\n",
    "    df_1 = df_1.drop('WeekNumber',axis=1)\n",
    "    filter_1 = ['AgencyId','ChannelId','RouteId','ClientId','ProductId','Sales_L1','SalesPesos_L1','Returns_L1',\n",
    "                'ReturnsPesos_L1','Demand_L1']\n",
    "    df_1.columns = filter_1\n",
    "    \n",
    "    #lag 2\n",
    "    df_2 = filtered_data[run_week-2]\n",
    "    df_2 = df_2.drop('WeekNumber',axis=1)\n",
    "    filter_2 = ['AgencyId','ChannelId','RouteId','ClientId','ProductId','Sales_L2','SalesPesos_L2','Returns_L2',\n",
    "                'ReturnsPesos_L2','Demand_L2']\n",
    "    df_2.columns = filter_2\n",
    "    \n",
    "    df_3 = filtered_data[run_week-3]\n",
    "    df_3 = df_3.drop('WeekNumber',axis=1)\n",
    "    filter_3 = ['AgencyId','ChannelId','RouteId','ClientId','ProductId','Sales_L3','SalesPesos_L3','Returns_L3',\n",
    "                'ReturnsPesos_L3','Demand_L3']\n",
    "    df_3.columns = filter_3\n",
    "    \n",
    "    # merge the frames...\n",
    "    join_keys = ['AgencyId','ChannelId','RouteId','ClientId','ProductId']\n",
    "    df = pd.merge(df, df_1, how='outer', on=join_keys)\n",
    "    df = pd.merge(df, df_2, how='outer', on=join_keys)\n",
    "    df = pd.merge(df, df_3, how='outer', on=join_keys)\n",
    "    joined_data.append(df)\n",
    "    run_week -= 1\n",
    "\n",
    "t2 = time.time()\n",
    "print 'lagged data created in: ', t2-t1, ' seconds'\n",
    "\n",
    "# now merge joined_data into a single frame - df_train...\n",
    "for i, item in enumerate(joined_data):\n",
    "    if i == 0:\n",
    "        df_train = joined_data[i]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train,joined_data[i]],ignore_index=True)\n",
    "\n",
    "print 'loading complete in: ', time.time()-t0, ' seconds. Dataset now has:', len(df_train.index), ' records'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in Lookup Data\n",
    "Link to Client, Product and Agency to add additional potential features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading complete in:  0.40900015831  seconds\n"
     ]
    }
   ],
   "source": [
    "# load the lookup files\n",
    "t0 = time.time()\n",
    "product_lookup = \"engineered_producto_tabla.csv\"\n",
    "product_columns = ['ProductId','OriginalName','ProductName','ProductPieces','ProductWeight','SupplierCode']\n",
    "df_product = pd.read_csv(data_location + product_lookup,names=product_columns,skiprows=1)\n",
    "\n",
    "client_lookup = \"engineered_cliente_tabla.csv\"\n",
    "client_columns = ['ClientId','OriginalName','ClientName']\n",
    "df_client = pd.read_csv(data_location + client_lookup,names=client_columns,skiprows=1)\n",
    "\n",
    "agency_lookup = \"town_state.csv\"\n",
    "agency_columns = ['AgencyId','Town','State']\n",
    "df_agency = pd.read_csv(data_location + agency_lookup,names=agency_columns,skiprows=1)\n",
    "\n",
    "print 'loading complete in: ', time.time()-t0, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge complete in:  3.44099998474  seconds\n"
     ]
    }
   ],
   "source": [
    "# now we need to join the 3 engineered lookup tables to the training data...\n",
    "t0 = time.time()\n",
    "\n",
    "# let's join to the agency table to add in some features\n",
    "df_train = pd.merge(left=df_agency,right=df_train, how='right', left_on='AgencyId', right_on='AgencyId')\n",
    "\n",
    "# let's join to the product table on ProductId\n",
    "df_train = pd.merge(left=df_product,right=df_train, how='right', left_on='ProductId', right_on='ProductId')\n",
    "\n",
    "# let's join to the client table on ClientId\n",
    "df_train = pd.merge(left=df_client,right=df_train, how='right', left_on='ClientId', right_on='ClientId')\n",
    "\n",
    "print 'merge complete in: ', time.time()-t0, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ClientId' 'ClientName' 'ProductId' 'ProductName' 'ProductPieces'\n",
      " 'ProductWeight' 'SupplierCode' 'AgencyId' 'Town' 'State' 'WeekNumber'\n",
      " 'ChannelId' 'RouteId' 'Sales' 'SalesPesos' 'Returns' 'ReturnsPesos'\n",
      " 'Demand' 'Sales_L1' 'SalesPesos_L1' 'Returns_L1' 'ReturnsPesos_L1'\n",
      " 'Demand_L1' 'Sales_L2' 'SalesPesos_L2' 'Returns_L2' 'ReturnsPesos_L2'\n",
      " 'Demand_L2' 'Sales_L3' 'SalesPesos_L3' 'Returns_L3' 'ReturnsPesos_L3'\n",
      " 'Demand_L3']\n"
     ]
    }
   ],
   "source": [
    "# we'll lose the original name columns as they're not useful for us\n",
    "df_train = df_train.drop(['OriginalName_x','OriginalName_y'],axis=1)\n",
    "print df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now write out the engineered training file to disk\n",
    "df_train.to_csv(data_location + 'engineered_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading complete in:  5.5759999752  seconds\n"
     ]
    }
   ],
   "source": [
    "# let's load it back in as a test...\n",
    "train_file = \"engineered_train.csv\"\n",
    "\n",
    "# specify columns for the training data as spanish is not a strong point in London\n",
    "train_columns = ['ClientId','ClientName','ProductId','ProductName','ProductPieces','ProductWeight'\n",
    "                 ,'SupplierCode','AgencyId','Town','State','WeekNumber','ChannelId','RouteId'\n",
    "                 ,'Sales','SalesPesos','Returns','ReturnsPesos','Demand','Sales_L1','SalesPesos_L1'\n",
    "                 ,'Returns_L1','ReturnsPesos_L1','Demand_L1','Sales_L2','SalesPesos_L2','Returns_L2','ReturnsPesos_L2'\n",
    "                 ,'Demand_L2','Sales_L3','SalesPesos_L3','Returns_L3','ReturnsPesos_L3','Demand_L3']\n",
    "\n",
    "# load the training data in full, skip the column headings on the first rows\n",
    "t0 = time.time() \n",
    "df_train = pd.read_csv(data_location + train_file,names=train_columns,skiprows=1)\n",
    "print 'loading complete in: ', time.time()-t0, ' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ClientId ClientName  ProductId ProductName ProductPieces ProductWeight  \\\n",
      "0       215   FACULTAD       1240  Mantecadas           4p          125g    \n",
      "1       215   FACULTAD       1240  Mantecadas           4p          125g    \n",
      "2       215   FACULTAD       1240  Mantecadas           4p          125g    \n",
      "3       215   FACULTAD       1242     Donitas           6p          105g    \n",
      "4       215   FACULTAD       1242     Donitas           6p          105g    \n",
      "\n",
      "  SupplierCode  AgencyId                    Town             State    ...      \\\n",
      "0          BIM      2095  2175 TOLUCA AEROPUERTO  ESTADO DE MÉXICO    ...       \n",
      "1          BIM      2095  2175 TOLUCA AEROPUERTO  ESTADO DE MÉXICO    ...       \n",
      "2          BIM      2095  2175 TOLUCA AEROPUERTO  ESTADO DE MÉXICO    ...       \n",
      "3          BIM      2095  2175 TOLUCA AEROPUERTO  ESTADO DE MÉXICO    ...       \n",
      "4          BIM      2095  2175 TOLUCA AEROPUERTO  ESTADO DE MÉXICO    ...       \n",
      "\n",
      "   Sales_L2  SalesPesos_L2  Returns_L2  ReturnsPesos_L2  Demand_L2  Sales_L3  \\\n",
      "0       NaN            NaN         NaN              NaN        NaN       3.0   \n",
      "1       3.0          25.14         0.0              0.0        3.0       NaN   \n",
      "2       NaN            NaN         NaN              NaN        NaN       NaN   \n",
      "3       NaN            NaN         NaN              NaN        NaN       3.0   \n",
      "4       3.0          22.92         0.0              0.0        3.0       NaN   \n",
      "\n",
      "   SalesPesos_L3  Returns_L3  ReturnsPesos_L3  Demand_L3  \n",
      "0          25.14         0.0              0.0        3.0  \n",
      "1            NaN         NaN              NaN        NaN  \n",
      "2            NaN         NaN              NaN        NaN  \n",
      "3          22.92         0.0              0.0        3.0  \n",
      "4            NaN         NaN              NaN        NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
